import os 
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import AIMessage
from langchain_core.prompts import PromptTemplate

from .squemas import IngestionResult
from .prompts import template

# Importamos los nuevos esquemas del estado
from agents.tech_surveillance.state import GraphState, ReportSchema, GeneralInfo, CallInfo

# ... (definici√≥n de chat_model y extraction_llm sin cambios) ...
chat_model = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    api_key=os.environ.get("GEMINI_API_KEY"),
    temperature=0.7,
    convert_system_message_to_human=True
)
extraction_llm = chat_model.with_structured_output(IngestionResult)


def ingestion_node(state: GraphState) -> dict:
    """
    Se activa cuando se detecta un nuevo proyecto o convocatoria. 
    Extrae la informaci√≥n, detecta si es convocatoria y estructura el estado.
    """
    print("--- Ejecutando Nodo: Ingesta de Proyecto/Convocatoria ---")
    last_message = state["messages"][-1].content
    
    prompt_template = PromptTemplate.from_template(template, partial_variables={"last_message": last_message})
    prompt = prompt_template.format()
    
    try:
        # 1. Llama al LLM para extraer la informaci√≥n (Call + Project)
        ingestion_result: IngestionResult = extraction_llm.invoke(prompt)
        
        project_info = ingestion_result.project_info
        call_info_data = ingestion_result.call_info
        
        # 2. Construye la secci√≥n 'GeneralInfo' para nuestro nuevo estado
        general_info = GeneralInfo(
            project_title=project_info.title,
            project_description=project_info.description,
            keywords=project_info.keywords,
            duration_months=18, 
            main_entity="Nombre Entidad Ejemplo" # Ejemplo
        )

        # 3. Inicializa el esquema principal del reporte
        report_components_schema = ReportSchema(
            general_info=general_info
        )
        
        # 4. Prepara el mensaje de confirmaci√≥n
        confirmation_text = ""
        
        if call_info_data:
            confirmation_text += f"‚úÖ **Convocatoria Detectada:** {call_info_data.title}\n"
            if call_info_data.url:
                confirmation_text += f"üîó **URL:** {call_info_data.url}\n"
            confirmation_text += "\n"
            
        if ingestion_result.is_generated_project:
            confirmation_text += (
                f"‚ú® **Proyecto Generado:** Como no proporcionaste una descripci√≥n espec√≠fica, he dise√±ado un proyecto alineado con la convocatoria:\n\n"
                f"üìå **T√≠tulo:** {project_info.title}\n"
                f"üìù **Descripci√≥n:** {project_info.description}\n\n"
            )
        else:
            confirmation_text += (
                f"üìã **Proyecto Registrado:**\n\n"
                f"**T√≠tulo:** {project_info.title}\n"
                f"**Descripci√≥n:** {project_info.description}\n\n"
            )
            
        confirmation_text += f"üè∑Ô∏è **Palabras Clave:** {', '.join(project_info.keywords)}\n\n"
        confirmation_text += "Ahora, proceder√© a realizar una investigaci√≥n acad√©mica y a planificar el proyecto."

        # 5. Devuelve la nueva estructura del estado
        return {
            "report_components": report_components_schema,
            "call_info": call_info_data, # Guardamos la info de la convocatoria en el estado
            "messages": [AIMessage(content=confirmation_text)]
        }

    except Exception as e:
        print(f"--- Error en el Nodo de Ingesta: {e} ---")
        error_message = AIMessage(
            content="Hubo un problema procesando tu solicitud. Por favor, intenta nuevamente con m√°s detalles sobre la convocatoria o tu proyecto."
        )
        return {"messages": [error_message]}